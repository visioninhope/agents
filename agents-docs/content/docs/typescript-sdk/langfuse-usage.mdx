---
title: Using Langfuse for LLM Observability
sidebarTitle: Langfuse Usage
description: Complete guide to using Langfuse for LLM observability, tracing, and analytics in the Inkeep Agent Framework
keywords: Langfuse, LLM observability, tracing, OpenTelemetry, AI monitoring, token usage, model analytics
icon: "brand/Langfuse"
---

Langfuse is an open-source LLM engineering platform that provides specialized observability for AI applications, including token usage tracking, model performance analytics, and detailed LLM interaction tracing.

## Quick Start

### 1. Setup Langfuse Account

First, create a Langfuse account and get your API keys:

1. **Sign up** at [Langfuse Cloud](https://cloud.langfuse.com)
2. **Create a new project** in your Langfuse dashboard
3. **Get your API keys** from the project settings:
   - Public Key: `pk-lf-xxxxxxxxxx`
   - Secret Key: `sk-lf-xxxxxxxxxx`

### 2. Configure OpenTelemetry Collector

Add Langfuse as an exporter to your OTEL collector configuration in the [`agents-optional-local-dev`](https://github.com/inkeep/agents-optional-local-dev) repository:

```yaml
# otel-collector-config.yaml
exporters:
  # Export to Langfuse
  otlphttp/langfuse:
    endpoint: "https://us.cloud.langfuse.com/api/public/otel" # US region
    headers:
      Authorization: "Basic <BASE64_ENCODED_CREDENTIALS>"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlphttp/langfuse]
```

### 3. Generate Authentication Credentials

Langfuse requires Basic Authentication with base64-encoded credentials:

```bash
# Generate base64 encoded credentials
echo -n "pk-lf-YOUR_PUBLIC_KEY:sk-lf-YOUR_SECRET_KEY" | base64
```

### 4. Complete Configuration Example

Here's a complete OTEL collector configuration with Langfuse integration:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:  
  batch:
    send_batch_size: 1

exporters:
  # Export to SigNoz OTEL Collector
  otlp/signoz:
    endpoint: host.docker.internal:4317
    tls:
      insecure: true
  # Export to Jaeger
  otlp/jaeger:
    endpoint: host.docker.internal:24317
    tls:
      insecure: true
  # Export to Langfuse
  otlp/langfuse:
    endpoint: "https://us.cloud.langfuse.com/api/public/otel"
    headers:
      Authorization: "Bearer ${LANGFUSE_CREDENTIAL}"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp/signoz, otlp/jaeger, otlp/langfuse]
```

### 5. Start the Services

```bash
# From the root directory of the agents-optional-local-dev
docker compose up -d

```

## Architecture

The Langfuse integration works alongside your existing observability stack:

```
Application → OTEL Collector → Jaeger → Jaeger UI (http://localhost:16686)
                             → SigNoz → SigNoz UI (http://localhost:3080)
                             → Langfuse → Langfuse Dashboard
```

## Dataset setup and execution

Use the [Inkeep Agent Cookbook](https://github.com/inkeep/agents-cookbook) repository which provides ready-to-use scripts for creating and running Langfuse dataset evaluations programmatically.

#### 1. Clone the Agent Cookbook Repository

```bash
git clone https://github.com/inkeep/agent-cookbook.git
cd agent-cookbook/evals/langfuse-dataset-example
```

**Set up environment variables in a `.env` file:**
```bash
# Langfuse configuration (required for both scripts)
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# Chat API configuration (for dataset runner)
INKEEP_AGENTS_RUN_API_KEY=your_api_key
INKEEP_AGENTS_RUN_API_URL=your_chat_api_base_url

# Execution context (for dataset runner)
INKEEP_TENANT_ID=your_tenant_id
INKEEP_PROJECT_ID=your_project_id
INKEEP_GRAPH_ID=your_graph_id
```

#### 2. Initialize Dataset with Sample Data

Run the basic Langfuse example to initialize a dataset with sample user messages:

```bash
pnpm run langfuse-init-example
```

This script will:
- Connect to your Langfuse project
- Create a new dataset called "inkeep-weather-example-dataset" with sample dataset items

#### 3. Run Dataset Items to Generate Traces

Run dataset items to generate traces that can be evaluated:

```bash
pnpm run langfuse-run-dataset
```

This script will:
- Read items from your Langfuse dataset
- Execute each item against your weather graph
- Generate the data needed for evaluation

## Running LLM Evaluations in Langfuse Dashboard

Langfuse provides a powerful web interface for running LLM evaluations without writing code. You can create datasets, set up evaluators, and run evaluations directly in the dashboard.

### Accessing the Evaluation Features

1. **Log into your Langfuse dashboard**: https://cloud.langfuse.com
2. **Navigate to your project** where your agent traces are being collected
3. **Click "Evaluations"** in the left sidebar
4. **Click "Set up evaluator"** to begin creating evaluations

### Setting Up LLM-as-a-Judge Evaluators

#### Set Up Default Evaluation Model

Before creating evaluators, you need to configure a default LLM connection for evaluations:

<img
  src="/images/langfuse-llm-connection.png"
  alt="Langfuse LLM Connection setup showing OpenAI provider configuration with API key field and advanced settings"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9" }}
/>

**Setting up the LLM Connection:**

1. **Navigate to "Evaluator Library"** in your Langfuse dashboard
2. **Click "Set up"** next to "Default Evaluation Model"
3. **Configure the LLM connection**:
   - **LLM Adapter**: Select your preferred provider
   - **Provider Name**: Give it a descriptive name (e.g., "openai")
   - **API Key**: Enter your OpenAI API key (stored encrypted)
   - **Advanced Settings**: Configure base URL, model parameters if needed
4. **Click "Create connection"** to save

#### Navigate to Evaluator Setup

1. **Go to "Evaluations"** → **"Running Evaluators"**
2. **Click "Set up evaluator"** button
3. **You'll see two main steps**: "1. Select Evaluator" and "2. Run Evaluator"

#### Choose Your Evaluator Type

You have two main options:

## Option A: Langfuse Managed Evaluators

Langfuse provides a comprehensive catalog of **pre-built evaluators**

**To use a managed evaluator:**

1. **Browse the evaluator list** and find one that matches your needs
2. **Click on the evaluator** to see its description and criteria
3. **Click "Use Selected Evaluator"** button

#### Customizing Managed Evaluators for Dataset Runs

Once you've selected a managed evaluator, you can **edit it to target your dataset runs**. This is particularly useful for evaluating agent performance against known test cases.

### Example: Customizing a Helpfulness Evaluator

1. **Select the "Helpfulness" evaluator** from the managed list
2. Under **Target** select dataset runs
3. **Configure variable mapping**
   - **{`{{input}}`}** → **Object**: Trace, **Object Variable**: Input
   - **{`{{generation}}`}** → **Object**: Trace, **Object Variable**: Output

<img
  src="/images/langfuse-helpfulness-setup.png"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9", maxWidth: "1600px", height: "auto", minHeight: "300px" }}
/>



## Option B: Create Custom Evaluator



1. **Click "+ Create Custom Evaluator"** button
2. **Fill in evaluator details**:

   - **Name**: Choose a descriptive name (e.g., "weather_tool_used")
   - **Description**: Explain what this evaluator measures
   - **Model**: Select evaluation model
   - **Prompt**: Configure a custom prompt


### Example: Customizing a Weather Tool Evaluator

1. **Prompt**

```
You are an expert evaluator for an AI agent system.
Your task is to rate the correctness of tool usage on a scale from 0.0 to 1.0.

Instructions:

If the user’s question is not weather-related and the tool used is not get_weather_forecast, return 1.0.

If the user’s question is not weather-related and the tool is get_weather_forecast, return 0.0.

If the user’s question is weather-related, return 1.0 only if the tool used is get_weather_forecast; otherwise return 0.0.

Input:
User Question: {`{{input}}`}
Tool Used: {`{{tool_used}}`}
```
<img
  src="/images/custom-eval.png"
  width="80%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9", maxWidth: "800px", height: "auto" }}
/>

2. **Configure variable mapping**:
   - **{`{{input}}`}** → **Object**: Trace, **Object Variable**: Input
   - **{`{{tool_used}}`}** → **Object**: Span, **Object Name**: weather-forecaster.ai.toolCall, **Object Variable**: Metadata, **JsonPath**: $.attributes["ai.toolCall.name"]

<img
  src="/images/custom-variable-mapping.png"
  alt="Langfuse helpfulness evaluator setup screen showing evaluator configuration with variable mapping and trace targeting options"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9", maxWidth: "1200px", height: "auto", minHeight: "300px" }}
/>


## Enable and Monitor

1. **Click "Enable Evaluator"** to start automatic evaluation
2. **Monitor evaluation progress** in the dashboard
3. **View evaluation results** as they complete
